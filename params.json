{
  "name": "KNN-Algorithm",
  "tagline": "Machine learning using K-NN Algorithm",
  "body": "### Introduction\r\nMost of the time, we love to ask for recommendation from our friends before experimenting new things. We usually start with those whose taste we feel we share and we deduce that we will like too. This is the basic intuition behind k-Nearest Neighbors Algorithm. Although this is perhaps one of the simplest machine learning algorithms, it is still used widely.  \r\n\r\n### Detailed Design\r\nLets check how we can implement this algorithm using R.\r\nFirst we will read the data and visualize using scatter plot.\r\n```r\r\n   library(ggplot2)\r\n   knn.df <- read.csv(\"dummy_data.csv\")\r\n   head(knn.df)\r\n   knn.df$Y <- factor(knn.df$Y)\r\n   ggplot(knn.df, aes(x=X1, y=X2, shape=Y, color=Y)) +\r\n     geom_point()\r\n```\r\n\r\n![knn-scatterplot](https://raw.githubusercontent.com/trisha87/kNN-algo/gh-pages/images/knn-scatterplot.png)\r\n\r\nNow we want to classify whether a new data(X1 and X2 given) will be assigned to 0 or 1 for Y. For this, we have to calculate the distance vector for this test data.\r\n```r\r\ndist.vectr <- function(knn.df, testdata)\r\n{\r\n  dist <- vector(, nrow(knn.df))\r\n  for(i in 1:nrow(knn.df))\r\n  {\r\n    dist[i] <- sqrt((knn.df[i, 'X1'] - testdata[1])^2 + (knn.df[i, 'X2'] - testdata[2])^2)\r\n  }\r\n  return(dist)\r\n}\r\n```\r\nOnce we have distance vector ready, we will require another function which will return the nearest data point. First we will sort the distance vector and get the first k nearest entries\r\n```r\r\nknn.points <- function(knn.df, k=5)\r\n{\r\n  dist <- dist.vectr(knn.df, testdata)\r\n  indices <- order(dist)[1:k]\r\n  predicted.y <- ifelse(mean(knn.df[indices, 'Y']) > 0.5, 1, 0)\r\n  return(predicted.y)\r\n}\r\n```\r\n\r\n### Using R Package\r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}